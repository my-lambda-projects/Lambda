<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, user-scalable=yes"
    />
    <title>d2</title>
    <style type="text/css">
      code {
        white-space: pre-wrap;
      }
      span.smallcaps {
        font-variant: small-caps;
      }
      span.underline {
        text-decoration: underline;
      }
      div.column {
        display: inline-block;
        vertical-align: top;
        width: 50%;
      }
    </style>
  </head>
  <body>
    <h1 id="d2">D2</h1>
    <p>
      {% embed
      url=“https://gist.github.com/bgoonz/3deac2c8f0dfedb503eefc78a4dba394”
      caption="" %}
    </p>
    <h2 id="hash-tables">Hash Tables</h2>
    <p>
      Hash tables (also known as hash maps) are associative arrays, or
      dictionaries, that allow for fast insertion, lookup and removal regardless
      of the number of items stored.
    </p>
    <p>Here’s an example of how they can be used:</p>
    <pre class="text"><code>HashTable phoneBook = new HashTable()
phoneBook.put(&quot;Adam&quot;, &quot;(202) 555-0309&quot;)
phoneBook.put(&quot;Beth&quot;, &quot;(335) 754-1215&quot;)

print(&quot;Adam’s number: &quot; + phoneBook.get(&quot;Adam&quot;))</code></pre>
    <p>
      Internally they are similar to card indexes: An item can be found quickly
      by first jumping to its approximate location, and then searching locally
      from there.
    </p>
    <p><img src="../.gitbook/assets/image.png" /></p>
    <h3 id="representation">Representation</h3>
    <p>
      The simplest way to implement a hash table is to use an
      <strong>array of linked lists</strong>.
    </p>
    <p>
      Each array cell is called a <strong>bucket</strong>, and each list node
      stores a <strong>key-value pair</strong>.
    </p>
    <p>
      Following the analogy from the previous section, the array cells that can
      be accessed quickly can be thought of as index cards, and nodes in the
      list as data cards.
    </p>
    <p><img src="../.gitbook/assets/image%20%281%29.png" /></p>
    <h3 id="finding-the-right-bucket">Finding the right bucket</h3>
    <p>
      Which bucket a given key belongs to, is determined by a
      <strong>hash function</strong> as follows…
    </p>
    <p>
      <em>bucket index</em> = hash(<em>key</em>) % length(<em>bucket array</em>)
    </p>
    <p>…where % denotes the remainder operator.</p>
    <p>
      For efficiency reasons, an additional variable is used to keep track of
      the current total number of key-value pairs.☞
    </p>
    <p>
      There are other ways to implement hash tables, but this is the
      representation that will be used here to introduce the subject. See
      section
      <a href="https://programming.guide/hash-tables.html#other-impl"
        >Other Collision Resolution Strategies</a
      >
      for alternatives.
    </p>
    <h2 id="insertion">Insertion</h2>
    <p>
      Here’s how a mapping from key <code>k</code> to value <code>v</code> is
      inserted in a hash table:
    </p>
    <p>
      <strong
        >1.Rehash if load factor is too high. (Explained in separate
        section.)</strong
      >
    </p>
    <p>rehash_if_needed()</p>
    <p><strong>2.Compute the hash for the key</strong></p>
    <p>h = hash(k)</p>
    <p><strong>3.Use remainder operator to compute the bucket index</strong></p>
    <p>i = h % length(buckets)</p>
    <p>
      <strong
        >4.Look for key among existing nodes. If found update value of existing
        node.</strong
      >
    </p>
    <p>****</p>
    <p>****</p>
    <p>****</p>
    <h3 id="lookup">Lookup</h3>
    <p>
      Here’s how the value for a key <code>k</code> is retrieved from a hash
      table:1.Compute the hash for the keyh = hash(k)2.Use remainder operator to
      compute the bucket indexi = h % length(buckets)3.Search through bucket
      linearlyn = buckets[i]<br />
      while n ≠ NULL:<br />
      if n.key == k:<br />
      return n.value<br />
      n = n.next<br />
      return NOT_FOUNDWrongkeyKeyfoundReturnvaluek,vk,vk,v
    </p>
    <h3 id="remove">Remove</h3>
    <p>
      Here’s how a key-value pair is removed from a hash table:1.Compute the
      hash for the keyh = hash(k)2.Use remainder operator to compute the bucket
      indexi = h % length(buckets)3.Search through bucket linearly to find
      keylast = NULL<br />
      n = buckets[i]<br />
      while n ≠ NULL:<br />
      if n.key == k:<br />
      if last == NULL:<br />
      buckets[i] = n.next<br />
      else:<br />
      last.next = n.next<br />
      n = n.next
    </p>
    <h3 id="rehashing">Rehashing</h3>
    <p>
      As the lengths of the linked lists grow, the average lookup time
      increases.
    </p>
    <p>
      To keep the linked lists short and lookups fast, the number of buckets
      must be increased (and nodes redistributed). This is called
      <strong>rehashing</strong>.1.Allocate a new bucket array.new_size = 2 *
      length(buckets)<br />
      new_buckets = new Bucket[new_size]2.Reinsert all elements in new array.for
      i in 0..buckets.length - 1:<br />
      while buckets[i] ≠ NULL:<br />
      n = buckets[i]<br />
      buckets[i] = n.next<br />
      h = hash(n.k)<br />
      i = h % new_length<br />
      n.next = new_buckets[i]<br />
      new_buckets[i] = n<br />
      3.Replace old array with new arraybuckets = new_bucketsbucketsnew_buckets
    </p>
    <p>
      During rehashing the number of buckets is increased by some factor,
      typically 2. As shown in the article
      <a href="https://programming.guide/hash-tables-complexity.html"
        >Hash Tables: Complexity</a
      >, it is important that the growth is exponential.
    </p>
    <h3 id="traversal">Traversal</h3>
    <p>
      1.For each bucket…for i in 0..buckets.length - 1:2.…traverse each node n =
      buckets[i]:<br />
      while n ≠ NULL:<br />
      process(n)<br />
      n = n.next
    </p>
    <p>
      Note that the order in which the key-value pairs are traversed is
      <strong>unpredictable</strong> due to the nature of hashing and rehashing.
    </p>
    <h3 id="load-factor-and-capacity">Load Factor and Capacity</h3>
    <p>
      The <strong>load factor</strong> is the average number of key-value pairs
      per bucket.load factor=total number of key-value pairsnumber of buckets
    </p>
    <p>
      It is when the load factor reaches a given limit that rehashing kicks in.
      Since rehashing increases the number of buckets, it reduces the load
      factor.
    </p>
    <p>
      The load factor limit is usually configurable and offers a
      <strong>tradeoff between time and space costs</strong>.Lower limitMore
      empty bucketsMore memory overheadHigher limitLarger bucketsSlower lookups
    </p>
    <p>
      The default load factor for a Java
      <a href="https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html"
        ><code>HashMap</code></a
      >
      is 0.75 and for a C#
      <a
        href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.hashtable?view=netframework-4.8"
        ><code>Hashtable</code></a
      >
      it’s 1.0.
    </p>
    <p>
      The <strong>capacity</strong> is the maximum number of key-value pairs for
      the given load factor limit and current bucket count.capacity=number of
      buckets×load factor limit
    </p>
    <p>
      Since rehashing increases the number of buckets, it increases the
      capacity.
    </p>
    <p>
      The default initial capacity for a Java
      <a href="https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html"
        ><code>HashMap</code></a
      >
      is 12 and for a C#
      <a
        href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.hashtable?view=netframework-4.8"
        ><code>Hashtable</code></a
      >
      it’s 0, i.e. the bucket array is initialized lazily upon first insertion.
    </p>
    <p>
      <strong>Example:</strong> Here’s the structure of a hash table, configured
      with load factor limit of 4.Currentload factor:24 / 8 =
      3Configuredlimit:4Current capacity:8 × 4 = 32
    </p>
    <h3 id="complexity-analysis">Complexity Analysis</h3>
    <p>
      As is clear from the way insert, lookup and remove works, the run time is
      proportional to the length of the linked lists. In worst case all keys
      hash to the same bucket, i.e. the whole data structure becomes equivalent
      to a linked list. This means that all operations run in
      <em>O</em>(<em>n</em>).
    </p>
    <p>
      Assuming however that keys are dispersed evenly among the buckets, it can
      be shown that the complexity of the expected run time is <em>O</em>(1) for
      all operations.
    </p>
    <p>
      See article
      <a href="https://programming.guide/hash-tables-complexity.html"
        >Hash Tables: Complexity</a
      >
      for a detailed analysis.
    </p>
    <h3 id="other-collision-resolution-strategies">
      Other Collision Resolution Strategies <a id="other-impl"></a>
    </h3>
    <p>
      When two keys hash to the same bucket, i.e. whenhash(_k_1) ≡ hash(_k_2)
      (mod number of buckets)
    </p>
    <p>
      it’s called a <strong>collision</strong>. The collision handling strategy
      described so far (one linked list per bucket) is an example of
      <strong>closed addressing</strong> using separate chains. Instead of
      linked lists, one can also use binary search trees, or as in the case of
      Java 9, a linked list up to a certain limit, and then convert it to a BST
      when more elements are added.
    </p>
    <p>
      The alternative, <strong>open addressing</strong>, is to store all
      key-value pairs directly in the hash table array, i.e. there’s at most one
      element per bucket. (The size of the array must always be at least as
      large as the number of elements stored.)
    </p>
    <h2 id="hash-tables-complexity">Hash Tables: Complexity</h2>
    <p>
      This article is written with separate chaining and closed addressing in
      mind, specifically implementations based on
      <strong>arrays of linked lists</strong>. Most of the analysis however
      applies to other techniques, such as basic open addressing
      implementations.☞
    </p>
    <p>
      Only operations that scale with the number of elements <em>n</em> are
      considered in the analysis below. In particular, the hash function is
      assumed to run in constant time.
    </p>
    <h3 id="length-of-chains">Length of chains</h3>
    <p>
      As is clear from the way lookup, insert and remove works, the run time is
      proportional to the number of keys in the given chain. So, to analyze the
      complexity, we need to analyze the length of the chains.
    </p>
    <h4 id="worst-case">Worst Case</h4>
    <p>
      If we’re unlucky with the keys we encounter, or if we have a poorly
      implemented hash function, all keys may hash to the same bucket.
    </p>
    <p>
      This means that the
      <strong
        >worst-case complexity of a hash table is the same as that of a linked
        list</strong
      >: <em>O</em>(<em>n</em>) for insert, lookup and remove.
    </p>
    <p>
      This is however a pathological situation, and the theoretical worst-case
      is often uninteresting in practice. When discussing complexity for hash
      tables the focus is usually on <strong>expected</strong> run time.
    </p>
    <h4 id="uniform-hashing">Uniform Hashing</h4>
    <p>
      The expected length of any given linked list depends on how the hash
      function spreads out the keys among the buckets. For the purpose of this
      analysis, we will assume that we have an ideal hash function. This is a
      common assumption to make. So common in fact, that it has a name:
    </p>
    <h4 id="simple-uniform-hashing-assumption">
      Simple Uniform Hashing Assumption
    </h4>
    <p>
      In a hash table with <em>m</em> buckets, each key is hashed to any given
      bucket…
    </p>
    <ul>
      <li>…with the same probability, 1 / <em>m</em></li>
      <li>…independently of which bucket any other key is hashed to.</li>
    </ul>
    <p>
      With SUHA the keys are distributed <strong>uniformly</strong> and the
      expected length of any given linked list is therefore <em>n</em> /
      <em>m</em>.
    </p>
    <h4 id="the-magic-part">The Magic Part</h4>
    <p>
      As you may recall, the <em>n</em> / <em>m</em> ratio is called the load
      factor, and that rehashing guarantees that this is bound by the configured
      load factor limit. (See
      <a
        href="https://programming.guide/hash-table-load-factor-and-capacity.html"
        >Hash Table Load Factor and Capacity</a
      >.) Since the load factor limit is constant,
      <strong
        >the expected length of all chains can be considered constant</strong
      >.☞
    </p>
    <p>
      <strong>A common misconception</strong> is that SUHA implies constant time
      worst case complexity. SUHA however, does not say that all keys
      <em>will</em> be distributed uniformly, only that the probability
      distribution is uniform. Even with a uniform probability, it is still
      <em>possible</em> for all keys to end up in the same bucket, thus worst
      case complexity is still linear.
    </p>
    <h3 id="insert">Insert</h3>
    <p>
      An insertion will search through one bucket linearly to see if the key
      already exists. This runs in <em>O</em>(<em>n</em> / <em>m</em>) which we
      know from the previous section is <em>O</em>(1). If the key is found, a
      value is updated, if not, a new node is appended to the list. Regardless
      of which, this part is in <em>O</em>(1).
    </p>
    <p>
      If we’re unlucky, rehashing is required before all that. Since rehashing
      performs <em>n</em> constant time insertions, it runs in Θ(<em>n</em>).
    </p>
    <p>
      That being said, rehashes are rare. In fact, they are so rare that
      <em>in average</em> insertion still runs in constant time. We say that the
      <strong>amortized time complexity for insert is</strong>
      <em><strong>O</strong></em
      ><strong>(1)</strong>.
    </p>
    <p>
      <strong>Proof:</strong> Suppose we set out to insert <em>n</em> elements
      and that rehashing occurs at each power of two. Let’s assume also that
      <em>n</em> is a power of two so we hit the worst case scenario and have to
      rehash on the very last insertion.
    </p>
    <p>
      We would have to rehash after inserting element 1, 2, 4, …, <em>n</em>.
      Since each rehashing reinserts all current elements, we would do, in
      total, 1 + 2 + 4 + 8 + … + <em>n</em> = 2n − 1 extra insertions due to
      rehashing. In other words, all rehashing necessary incurs an average
      overhead of less than 2 extra insertions per element.
    </p>
    <p>
      We conclude that despite the growing cost of rehashing, the average number
      of insertions per element stays constant.
    </p>
    <h3 id="lookup-1">Lookup</h3>
    <p>
      A lookup will search through the chain of one bucket linearly. This is in
      <em>O</em>(<em>n</em> / <em>m</em>) which, again, is
      <em><strong>O</strong></em
      ><strong>(1)</strong>.
    </p>
    <h3 id="removal">Removal</h3>
    <p>
      A removal will search through one bucket linearly. If the key is found, it
      is “unlinked” in constant time, so remove runs in
      <em><strong>O</strong></em
      ><strong>(1)</strong> as well.
    </p>
    <p>
      If one wants to reclaim unused memory, removal may require allocating a
      smaller array and rehash into that. In this case removal runs in
      <em>O</em>(<em>n</em>) in worst case, and <em>O</em>(1) amortized.
    </p>
    <h3 id="traversal-1">Traversal</h3>
    <p>
      There’s no way to know which buckets are empty, and which ones are not, so
      all buckets must be traversed. This means traversal is Θ(<em>n</em> +
      <em>m</em>).
    </p>
    <p>
      In practice this is only relevant if the hash table is initialized with a
      very large capacity. After the first rehashing the number of buckets can
      be considered linearly proportional to the number of items, and traversal
      is <em>Θ</em>(<em>n</em>).
    </p>
    <h2 id="inked-hash-table">inked Hash Table</h2>
    <p>
      A linked hash table is a
      <strong>combination of a hash table and a linked list</strong>. Nodes are
      arranged in buckets but also have a doubly linked list running through
      them.
    </p>
    <p>
      One can use the hash table structure for fast hash based lookups, and the
      linked list for effirient traversal.
    </p>
    <h3 id="hash-table-traversal">Hash Table Traversal</h3>
    <p>
      There’s no way to know which buckets are empty, and which ones are not, so
      all buckets must be traversed. This means traversal is Θ(<em>n</em> +
      <em>m</em>).
    </p>
    <p>
      In practice this is only relevant if the hash table is initialized with a
      very large capacity. After the first rehashing the number of buckets can
      be considered linearly proportional to the number of items, and traversal
      is <em>Θ</em>(<em>n</em>).
    </p>
    <h3 id="adding-a-linked-list">Adding a linked list</h3>
    <p>
      If one wants to avoid the overhead due to empty buckets one can let a
      linked list run through all nodes. Whenever a node is added to the hash
      table, it’s appended to this list.
    </p>
    <p>
      <strong>Example:</strong> A linked hash table based on separate chaining.
    </p>
    <p>
      This effectively trades some memory (additional next / prev pointers) for
      improved speed.
    </p>
    <p>
      Since the overlay list structure is doubly linked, append and remove are
      constant time operations, so it does not affect the complexity of other
      operations.
    </p>
    <p>
      The
      <a
        href="https://docs.oracle.com/javase/8/docs/api/java/util/LinkedHashMap.html"
        ><code>LinkedHashMap</code></a
      >
      in the Java API is an example of this technique.
    </p>
  </body>
</html>
